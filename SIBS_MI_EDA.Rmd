---
title: "Exploratory Data Analysis"
author: "Carson Slater"
date: '2022-07-14'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

```{r, include=FALSE}
library(tidymodels)
library(knitr)
library(readr)
```

```{r, include=FALSE}
# Import Data
mi_orig_data <- read_csv("hackathon_data.csv")
```

```{r, include=FALSE}
# Added variables created by Owen Wagner (Total_NSIDS, Complications)
urlfile = "https://raw.githubusercontent.com/carsonslater/sibs_summer2022/Main/Myocardial_infarction_complications_Database2.csv"

mi_2 <- read.csv(url(urlfile))
```

```{r, include=FALSE}
# Cleaning the Data
missing_prop <- colMeans(is.na(mi_2))

# Choosing to remove columns with greater than 40% of the data missing
mi_2 <- mi_2 %>% select(-IBS_NASL, -S_AD_KBRIG, -D_AD_KBRIG, -KFK_BLOOD) %>% 
  mutate(Complication = as.factor(Complication))
```

```{r, include=FALSE}
# Preliminary work to sort people into 'buckets'
# We decided, in order to measure the impact of NSADIS, we spitt the data into 'buckets' of people who have had certain combinations of NSAIDS

NSAIDS_day1_only <- mi_orig_data %>% filter(NOT_NA_1_n != 0, NOT_NA_2_n == 0, NOT_NA_3_n == 0)
NSAIDS_day2_only <- mi_orig_data %>% filter(NOT_NA_1_n == 0, NOT_NA_2_n != 0, NOT_NA_3_n == 0)
NSAIDS_day3_only <- mi_orig_data %>% filter(NOT_NA_1_n == 0, NOT_NA_2_n == 0, NOT_NA_3_n != 0)
NSAIDS_day12 <- mi_orig_data %>% filter(NOT_NA_1_n != 0, NOT_NA_2_n != 0, NOT_NA_3_n == 0)
NSAIDS_day13 <- mi_orig_data %>% filter(NOT_NA_1_n != 0, NOT_NA_2_n == 0, NOT_NA_3_n != 0)
NSAIDS_day23 <- mi_orig_data %>% filter(NOT_NA_1_n == 0, NOT_NA_2_n != 0, NOT_NA_3_n != 0)
NSAIDS_day123 <- mi_orig_data %>% filter(NOT_NA_1_n != 0, NOT_NA_2_n != 0, NOT_NA_3_n != 0)
NSAIDS_none <- mi_orig_data %>% filter(NOT_NA_1_n != 0, NOT_NA_2_n != 0, NOT_NA_3_n != 0)

```

```{r, include=FALSE}
set.seed(145)

# Split the data
mi_split <- initial_split(mi_2, prop = 0.75)
mi_train <- training(mi_split)
mi_test <- testing(mi_split)

# Make Cross Validation Folds
mi_folds <- vfold_cv(mi_train, v = 10)
```

## Building A Logistic Regression Model
```{r}
log_reg_mod <- logistic_reg(
  mode = "classification",
  engine = "glmnet",
  penalty = 0.001,
  mixture = 0
)

log_reg_rec <- recipe(Complication ~., data = mi_2) %>% 
  step_impute_knn(all_numeric_predictors())

log_reg_wkflow <- workflow() %>% 
  add_recipe(log_reg_rec) %>% 
  add_model(log_reg_mod)
```

## Fitting the Logistic Regression Model
```{r}
log_reg_fit <- fit(log_reg_wkflow, data = mi_2)
tidy(log_reg_fit)
```

# Evaluating the Logistic Regression Model
```{r}
# Augmented model
model_aug <- augment(log_reg_fit, truth = Complication, 
                     new_data = mi_2)
# roc score
auc_score <- roc_auc(data = model_aug, truth = Complication, 
                     estimate = .pred_1, event_level = "second")
# plot of roc curve
autoplot(roc_curve(data = model_aug, truth = Complication,
                   estimate = .pred_1, event_level = "second"))
```
```

